---
title: "Feed-Forward Neural Networks"
subtitle: "Perceptrons, MLPs and SGD"
author: "Miguel Fonseca"
format:
  revealjs:
    toc: true
---

# Motivation & intuition

## Why Neural Networks?

- Universal function approximators  
- Flexible, data-driven models  
- Strong performance in:
  - Vision
  - NLP
  - Finance
  - Biomedicine

# Single Perceptron

## The Perceptron: Core Idea

- Linear classifier
- Inspired by biological neurons
- Computes a **weighted sum** + bias

$$
\widehat{y} = \mathbb{1}(w^T x + b > 0)
$$

## Perceptron Learning Rule

- Update weights on mistakes

$$
w \leftarrow w + \eta (y - \widehat{y}) x
$$

- Online algorithm  
- Converges if data is linearly separable



# Feed-Forward Neural Networks

## Limitation of a Single Perceptron

- Can only learn **linear decision boundaries**
- Cannot solve XOR

![](/images/xor.png){width=70%}

---

## Universal Approximation Theorem {.scrollable}

> A feed-forward network with one hidden layer can approximate any continuous function on a compact domain.

Expressiveness grows with:

- Number of neurons
- Depth
- Choice of activation

---

## Feed-Forward Neural Networks

- Stack perceptrons into **layers**
- Information flows left → right

$$
x \rightarrow h^{(1)} \rightarrow h^{(2)} \rightarrow \widehat{y}
$$

---

![](/images/hidden_layer_definition.png)

---

## Multilayer Perceptron (MLP)

- Input layer
- One or more **hidden layers**
- Output layer

Each neuron:
$$
h_j = \sigma(w_j^T x + b_j)
$$

---

## Why Hidden Layers Matter

- Enable non-linear representations
- Compose simple functions → complex ones
- Depth vs width trade-off

![](https://upload.wikimedia.org/wikipedia/commons/1/10/Deep_neural_network.svg)

# Activation Functions

## Why Activations Matter

::: {.columns}
:::: {.column width="50%"}

### Without activations

- Network collapses to a linear model

::::

:::: {.column width="50%"}

### With activations

- Introduce non-linearity
- Control gradient flow

::::
:::

---

## Activation Functions

Purpose:

- Introduce **non-linearity**
- Control gradient flow

Common choices:

- Sigmoid
- Tanh
- ReLU
- Softmax

---

## Sigmoid & Tanh

Sigmoid:
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

Tanh:
$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

- Smooth
- Saturation → vanishing gradients

---


![Sigmoid & tanh Activation Functions](/images/sigmoid_tanh_activation_functions.png)

---

## ReLU Family

ReLU:
$$
\operatorname{ReLU}(x) = \max(0, x)
$$

Variants:

- Leaky ReLU
- ELU

Pros:

- Sparse activations
- Faster training

---

![](https://upload.wikimedia.org/wikipedia/commons/f/fe/Activation_rectified_linear.svg)

---

## Output Activations

- Regression: **Identity**
- Binary classification: **Sigmoid**
- Multiclass: **Softmax**

$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

# Loss Functions

## Loss Functions: Why Do We Need Them?

A neural network produces predictions:

$$
\hat{y} = f_\theta(x)
$$

But how do we measure *how wrong* it is?

We define a **loss function**:

$$
\mathscr{L}(y, \widehat{y})
$$

Training objective:

$$
\min_\theta \; \mathscr{L}(y, \widehat{y})
$$

The loss defines:
- The optimization problem
- The gradient signal
- The statistical interpretation

---

## Regression Loss: Mean Squared Error (MSE)

Used for:
- Regression
- Continuous targets

Definition:

$$
\mathscr{L}_{MSE}
=
\frac{1}{n} \sum_{i=1}^n (y_i - \widehat{y}_i)^2
$$

Properties:
- Penalizes large errors heavily
- Smooth and differentiable
- Corresponds to Gaussian noise assumption

---

## MSE: Gradient Intuition

For a single example:

$$
\mathscr{L} = \frac{1}{2}(y - \widehat{y})^2
$$

Derivative:

$$
\frac{\partial \mathscr{L}}{\partial \widehat{y}}
=
\widehat{y} - y
$$

Interpretation:
- If prediction too large → positive gradient
- If prediction too small → negative gradient

This becomes the **error signal** in backpropagation

---

## Binary Classification: Binary Cross-Entropy

Used for:
- Spam detection
- Credit default
- Medical diagnosis

Output: probability

$$
\hat{y} = \sigma(z)
$$

Loss:

$$
\mathscr{L}_{BCE}
=
- \left[
y \log(\widehat{y})
+
(1-y)\log(1-\widehat{y})
\right]
$$

---

## Why Not Use MSE for Classification?

Problem:

- Sigmoid saturates
- Gradients become very small

Binary cross-entropy:

- Strong gradients when wrong
- Better probabilistic interpretation

Key result:

With sigmoid + BCE:

$$
\frac{\partial \mathscr{L}}{\partial z}
=
\widehat{y} - y
$$

Clean gradient → stable training

---

## Multiclass Classification: Cross-Entropy

Used for:

- Digit recognition
- Image classification
- NLP tagging

Output: Softmax

$$
\hat{y}_i =
\frac{e^{z_i}}{\sum_j e^{z_j}}
$$

Loss:

$$
\mathscr{L}_{CE}
=
- \sum_{i=1}^K y_i \log(\widehat{y}_i)
$$

Where:

- $y_i$ is one-hot encoded

---

## Gradient Insight: Softmax + Cross-Entropy

Beautiful result:

$$
\frac{\partial \mathscr{L}}{\partial z_i}
=
\widehat{y}_i - y_i
$$

Same structure as:

- MSE gradient
- Binary cross-entropy gradient

Pattern:

> Error = Prediction − Target

This is what backpropagation propagates backward.

# Backpropagation

## Training a Neural Network

Goal:
$$
\min_\theta \; \mathscr{L}(y, \widehat{y})
$$

Steps:

1. Forward pass
2. Compute loss
3. Backpropagate gradients
4. Update parameters

---

## Backpropagation: Intuition

- Efficient application of the **chain rule**
- Propagates error backward
- Computes gradients for all weights

---

## Backpropagation: What Problem Are We Solving?

We want to compute efficiently:

$$
\frac{\partial \mathscr{L}}{\partial W^{(l)}} \quad \text{for all layers } l
$$

Naively:

- Many parameters
- Repeated computations

::: {.callout-important title="Key idea"}
- **Reuse intermediate derivatives**
- Apply the **chain rule systematically**
:::

---

## Backpropagation: Chain Rule View

Forward pass:
$$
x \rightarrow z^{(1)} \rightarrow h^{(1)} \rightarrow z^{(2)} \rightarrow \widehat{y}
$$

Where:
$$
z^{(l)} = W^{(l)} h^{(l-1)} + b^{(l)}
$$
$$
h^{(l)} = \sigma(z^{(l)})
$$

Backward pass:
$$
\frac{\partial \mathscr{L}}{\partial z^{(l)}} =
\frac{\partial \mathscr{L}}{\partial z^{(l+1)}}
\frac{\partial z^{(l+1)}}{\partial z^{(l)}}
$$

::: {.callout-note}
Errors flow **backwards**, data flows **forwards**
:::

---

## Backpropagation: Error Terms

Define the **error signal** at layer $l$:

$$
\delta^{(l)} = \frac{\partial \mathscr{L}}{\partial z^{(l)}}
$$

Output layer:
$$
\delta^{(L)} =
\nabla_{\widehat{y}} \mathscr{L} \odot \sigma'(z^{(L)})
$$

Hidden layers:
$$
\delta^{(l)} =
(W^{(l+1)})^T \delta^{(l+1)} \odot \sigma'(z^{(l)})
$$

Gradients:
$$
\frac{\partial \mathscr{L}}{\partial W^{(l)}} =
\delta^{(l)} (h^{(l-1)})^T
$$

---

## Toy Example: One Hidden Neuron

Model:

- Input: $x = 1$
- Hidden: $h = \sigma(w_1 x)$
- Output: $\widehat{y} = w_2 h$
- Loss: $\mathscr{L} = \frac{1}{2}(y - \widehat{y})^2$

Forward:
$$
h = \sigma(w_1)
$$

$$
\widehat{y} = w_2 h
$$

Backward:

$$
\frac{\partial \mathscr{L}}{\partial w_2}
= (\widehat{y} - y) h
$$

$$
\frac{\partial \mathscr{L}}{\partial w_1}
= (\widehat{y} - y) w_2 \sigma'(w_1)
$$

:::{.callout-note}
Error at output **propagates backward** to earlier weights
:::

---

## Stochastic Gradient Descent (SGD)

- Uses **mini-batches**
- Noisy but efficient

Variants:

- Momentum
- RMSProp
- Adam

---

![](/images/sgd.png)

# Examples & applications

## Example: Regression

- Predict house prices
- Inputs: size, location, age
- Output: price

MLP with:

- ReLU hidden layers
- MSE loss

---

## Example: Classification

- Spam detection
- Inputs: word frequencies
- Output: spam / not spam

MLP with:

- Sigmoid output
- Cross-entropy loss

---

## Example: Finance

**Applications:**

- Credit scoring
- Default probability
- Asset return prediction

**Caveats:**

- Overfitting
- Interpretability
- Data leakage
